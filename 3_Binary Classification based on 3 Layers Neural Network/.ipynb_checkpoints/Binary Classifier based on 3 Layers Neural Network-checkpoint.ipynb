{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 20165549\\ \\ JANG\\ \\ JAE\\ \\ YONG$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a binary classifier for human versus horse based on logistic regression using the dataset that consists of human and horse images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\n",
    "    # the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=False, num_workers=1)  \n",
    "\n",
    "validation_data_path = 'horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=1)  \n",
    "\n",
    "NUM_EPOCH = range(1) # my code\n",
    "\n",
    "#\n",
    "# Image size and number of data\n",
    "#\n",
    "image_size = 10000\n",
    "train_count = 1027\n",
    "validation_count = 256\n",
    "\n",
    "train_label = np.empty(train_count, dtype=float)\n",
    "train_data = np.ones((train_count, image_size), dtype=float)\n",
    "\n",
    "validation_label = np.empty(validation_count, dtype=float)\n",
    "validation_data = np.ones((validation_count, image_size), dtype=float)\n",
    "\n",
    "for epoch in (NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)\n",
    "        \n",
    "        train_label[i] = int(labels)\n",
    "        train_tmp = np.asfarray(inputs)\n",
    "        train_data[i, :10000] = train_tmp[0, 0, :, :].reshape(10000)\n",
    "        \n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)\n",
    "        \n",
    "        validation_label[i] = int(labels)\n",
    "        validation_tmp = np.asfarray(inputs)\n",
    "        validation_data[i, :10000] = validation_tmp[0, 0, :, :].reshape(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "learningRate = 10**(-5)\n",
    "delta = 10**(-9)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def loss_function(t, y):\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log(1-y+delta)) \n",
    "\n",
    "iteration = 5000\n",
    "\n",
    "#d = np.empty(train_count, dtype=float)\n",
    "\n",
    "u = np.empty((image_size, 2), dtype=float)\n",
    "v = np.empty((2, 3), dtype=float)\n",
    "w = np.empty(3, dtype=float)\n",
    "\n",
    "a = np.empty((train_count, 2), dtype=float)\n",
    "b = np.empty((train_count, 3), dtype=float)\n",
    "c = np.empty(train_count, dtype=float)\n",
    "h = np.empty(train_count, dtype=float)\n",
    "\n",
    "loss_arr = np.zeros(iteration, dtype=float)\n",
    "t_loss_arr = np.zeros(iteration, dtype=float)\n",
    "loss_value_arr = np.zeros(iteration, dtype=float)\n",
    "t_loss_value_arr = np.zeros(iteration, dtype=float)\n",
    "\n",
    "for i in range(image_size):\n",
    "    for j in range(2):\n",
    "        u[i, j] = np.random.rand(1)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        v[i, j] = np.random.rand(1)\n",
    "\n",
    "for i in range(3):\n",
    "    w[i] = np.random.rand(1)\n",
    "\n",
    "for i in range(train_count):\n",
    "    for j in range(2):\n",
    "        a[i, j] = u[:, j].dot(train_data[i, :])\n",
    "\n",
    "for i in range(train_count):\n",
    "    for j in range(3):\n",
    "        b[i, j] = v[:, j].dot(sigmoid(a[i, :]))\n",
    "\n",
    "for i in range(train_count):\n",
    "    c[i] = w[:].dot(sigmoid(b[i ,:]))\n",
    "\n",
    "for i in range(train_count):\n",
    "    h[i] = sigmoid(c[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 10\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 20\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 30\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 40\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 50\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 60\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 70\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 80\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 90\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 100\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 110\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 120\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 130\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 140\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 150\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 160\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 170\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 180\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 190\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 200\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 210\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 220\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 230\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 240\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 250\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 260\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 270\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 280\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 290\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 300\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 310\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 320\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 330\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 340\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 350\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 360\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 370\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 380\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 390\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 400\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 410\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 420\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 430\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 440\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 450\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 460\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 470\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 480\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 490\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 500\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 510\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 520\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 530\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 540\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 550\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 560\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 570\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 580\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 590\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 600\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 610\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 620\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 630\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 640\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 650\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 660\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 670\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 680\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 690\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 700\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 710\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 720\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 730\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 740\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 750\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 760\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 770\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 780\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 790\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n",
      "iter = 800\n",
      "train loss count = 500\n",
      "train loss value = 797.6995343712956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-04eec120b481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearningRate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_lc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_cb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_ba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-e772c734f296>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_lc = np.empty(train_count, dtype=float)\n",
    "d_cb = np.empty(train_count, dtype=float)\n",
    "d_ba = np.empty(train_count, dtype=float)\n",
    "\n",
    "for iter in range(iteration):\n",
    "    \n",
    "    for i in range(train_count):\n",
    "        d_lc[i] = (h[i] - train_label[i]) * h[i] * (1 - h[i])\n",
    "        d_cb[i] = w[:].dot(sigmoid(b[i, :]))\n",
    "        d_ba[i] = 0\n",
    "        for j in range(3):\n",
    "            d_ba[i] += v[:, j].dot(sigmoid(a[i, :]))\n",
    "        d_ba[i] /= 3\n",
    "        \n",
    "    for i in range(image_size):\n",
    "        for j in range(2):\n",
    "            u[i, j] = u[i, j] - learningRate * np.sum(d_lc[:] * d_cb[:] * d_ba[:] * sigmoid(train_data[:, i]))\n",
    "        \n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            v[i, j] = v[i, j] - learningRate * np.sum(d_lc[:] * d_cb[:] * sigmoid(a[:, i]))\n",
    "    \n",
    "    for i in range(3):\n",
    "        w[i] = w[i] - learningRate * np.sum(d_lc[:] * sigmoid(b[:, i]))\n",
    "    \n",
    "    loss_value = 0\n",
    "    t_loss_value = 0\n",
    "    training_loss = 0\n",
    "    testing_loss = 0\n",
    "    \n",
    "    for i in range(train_count):\n",
    "        loss_value += loss_function(train_label[i], h[i])\n",
    "\n",
    "        if train_label[i] == 1:\n",
    "            if h[i] <= 0.5: # loss count\n",
    "                training_loss += 1\n",
    "\n",
    "        else:\n",
    "            if h[i] > 0.5: # loss count\n",
    "                training_loss += 1\n",
    "\n",
    "#     for i in range(validation_count):\n",
    "#         t_loss_value += loss_function(validation_label[i], sigmoid(w[:].dot(validation_data[i, :])))\n",
    "#         if validation_label[i] == 1:\n",
    "#             if sigmoid(w[:].dot(validation_data[i, :])) <= 0.5: # loss count\n",
    "#                 testing_loss += 1\n",
    "\n",
    "#         else:\n",
    "#             if sigmoid(w[:].dot(validation_data[i, :])) > 0.5: # loss count\n",
    "#                 testing_loss += 1\n",
    "\n",
    "    if iter % 10 == 0:\n",
    "        print(\"iter =\", iter)\n",
    "        print(\"train loss count =\", training_loss)\n",
    "        print(\"train loss value =\", loss_value / train_count)\n",
    "#         print(\"validation loss count =\", testing_loss)\n",
    "#         print(\"validation loss value =\", t_loss_value)\n",
    "        \n",
    "    \n",
    "    loss_value_arr[iter] = loss_value\n",
    "#     t_loss_value_arr[iter] = t_loss_value\n",
    "    loss_arr[iter] = training_loss\n",
    "#     t_loss_arr[iter] = testing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y = loss_value_arr[x] / train_count\n",
    "\n",
    "# plt.plot(x, y, \"BLUE\")\n",
    "# plt.title('Training Loss')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y = (train_count - loss_arr[x]) / train_count\n",
    "\n",
    "# plt.plot(x, y, \"BLUE\")\n",
    "# plt.title('Training Accuracy')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Accuracy percent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Validation Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y = t_loss_value_arr[x] / validation_count\n",
    "\n",
    "# plt.plot(x, y, \"RED\")\n",
    "# plt.title('Validation Loss')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y = (validation_count - t_loss_arr[x]) / validation_count\n",
    "\n",
    "# plt.plot(x, y, \"RED\")\n",
    "# plt.title('Validation Accuracy')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Accuracy percent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training and Validation at every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y1 = loss_value_arr[x] / train_count\n",
    "# y2 = t_loss_value_arr[x] / validation_count\n",
    "\n",
    "# plt.plot(x, y1, \"BLUE\")\n",
    "# plt.plot(x, y2, \"RED\")\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(iteration)\n",
    "# y1 = (train_count - loss_arr[x]) / train_count\n",
    "# y2 = (validation_count - t_loss_arr[x]) / validation_count\n",
    "\n",
    "# plt.plot(x, y1, \"BLUE\")\n",
    "# plt.plot(x, y2, \"RED\")\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Accuracy percent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the table for the final accuracy and loss with training and validation datasets as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss = 0.10260832378422415\n",
      "Final Training Accuracy = 97.6630963972736 %\n",
      "Final Validation Loss = 0.9855954202361976\n",
      "Final Validation Accuracy = 75.78125 %\n"
     ]
    }
   ],
   "source": [
    "print ('Final Training Loss =', loss_value_arr[iteration - 1] / train_count)\n",
    "print ('Final Training Accuracy =', (train_count - loss_arr[iteration - 1]) / train_count * 100, '%')\n",
    "print ('Final Validation Loss =', t_loss_value_arr[iteration - 1] / validation_count)\n",
    "print ('Final Validation Accuracy =', (validation_count - t_loss_arr[iteration - 1]) / validation_count * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>0.102608</td>\n",
       "      <td>97.66 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.985595</td>\n",
       "      <td>75.78 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Loss Accuracy\n",
       "Training    0.102608  97.66 %\n",
       "Validation  0.985595  75.78 %"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "idx1 = round(loss_value_arr[iteration - 1] / train_count, 6)\n",
    "idx2 = str(round((train_count - loss_arr[iteration - 1]) / train_count * 100, 2)) + ' %'\n",
    "idx3 = round(t_loss_value_arr[iteration - 1] / validation_count, 6)\n",
    "idx4 = str(round((validation_count - t_loss_arr[iteration - 1]) / validation_count * 100, 2)) + ' %'\n",
    "\n",
    "df = pd.DataFrame(data=np.array([[idx1, idx2], [idx3, idx4]]), index= ['Training', 'Validation'], columns=['Loss', 'Accuracy'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MarkDown :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Loss | Accuracy |\n",
    "|:--------|:--------:|--------:|\n",
    "| Training | 0.102608 | 97.66% |\n",
    "| Validation | 0.985595 | 75.75% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
